"""
æ³•è¦åˆ†æç³»çµ± - é€šç”¨ç‰ˆ
æ”¯æ´å¤šç¨® AI æœå‹™ï¼šOllamaã€OpenAIã€Google Gemini
æ”¯æ´æœ¬åœ°è³‡æ–™åº«èˆ‡ä¸Šå‚³æª”æ¡ˆåˆ†æ
"""

import streamlit as st
from openai import OpenAI
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    genai = None
import requests
import re
import pathlib
import json
import pandas as pd
import io
from typing import List, Tuple, Dict, Any, Optional, Union, Set
from dataclasses import dataclass
from enum import Enum

# =============================
# é…ç½®é¡åˆ¥
# =============================

class AIProvider(Enum):
    """AI æœå‹™æä¾›è€…"""
    OLLAMA = "Ollama (æœ¬åœ°)"
    OPENAI = "OpenAI"
    GEMINI = "Google Gemini"

@dataclass
class Config:
    """ç³»çµ±é…ç½®"""
    BASE_DIR: pathlib.Path = pathlib.Path(__file__).parent
    
    # é è¨­è³‡æ–™å¤¾
    DEFAULT_DATABASES = {
        "åœ‹åœŸè¨ˆç•«æ³•è¦": BASE_DIR / "laws_txt",
        "éƒ½å¸‚è¨ˆç•«æ³•è¦": BASE_DIR / "doji_txt",
        "å…¨åœ‹æ³•è¦JSON": BASE_DIR / "mojLawSplitJSON",
    }
    
    # KEYWORDS_TXT: pathlib.Path = BASE_DIR / "test.txt"
    KEYWORDS_TXT: pathlib.Path = BASE_DIR / "MID_National_1030.txt"
    
    # Origin JSON è·¯å¾‘
    ORIGIN_JSON: pathlib.Path = BASE_DIR / "Origin" / "OriginBook1104.json"
    
    # Taide JSON è·¯å¾‘
    TAIDE_JSON: pathlib.Path = BASE_DIR / "Taide" / "Taide1105.json"
    
    # å¸¸æ•¸
    TXT_SOURCE_LABEL: str = "æ³•è¦è³‡æ–™åº«"
    JSON_SOURCE_LABEL: str = "å…¨åœ‹æ³•è¦è³‡æ–™åº«"
    AI_SOURCE_LABEL: str = "AIå»ºè­°"
    MAX_CTX_CHARS: int = 16384
    
    # Ollama è¨­å®š
    OLLAMA_BASE_URL: str = "http://127.0.0.1:11434"
    OLLAMA_API_KEY: str = "ollama"
    
    # API è¨­å®š
    OPENAI_API_KEY: str = ""
    GEMINI_API_KEY: str = ""
    
    TIMEOUT: int = 120

config = Config()

# =============================
# æ­£å‰‡è¡¨é”å¼æ¨¡å¼
# =============================

class RegexPatterns:
    """æ­£å‰‡è¡¨é”å¼æ¨¡å¼é›†åˆ"""
    SIGNALS_RE = re.compile(r"(æœ¬æ³•æ‰€ç¨±|æ‰€ç¨±|ç¨±ç‚º|ç¨±ä¹‹ç‚º|ä¿‚æŒ‡|æ˜¯æŒ‡|æŒ‡ç‚º|æŒ‡ç¨±|æ„æŒ‡|æ„è¬‚|æ„å³|è¬‚ç‚º|è¬‚ä¹‹|å®šç¾©|å®šç¾©å¦‚ä¸‹|æ¦‚ç¨±|æ³›æŒ‡)")
    ARTICLE_HEAD_RE = re.compile(r"^ç¬¬\s*([0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+)\s*æ¢", re.M)
    ENUM_ANCHOR_RE = re.compile(r"(åè©å®šç¾©|å®šç¾©å¦‚ä¸‹|æœ¬æ³•ç”¨èª[ï¼Œã€,]\s*å®šç¾©å¦‚ä¸‹|æœ¬æ³•ç”¨è©[ï¼Œã€,]\s*å®šç¾©å¦‚ä¸‹|æœ¬æ¢ç”¨èª[ï¼Œã€,]\s*å®šç¾©å¦‚ä¸‹|æœ¬æ³•æ‰€ç¨±)")
    LIST_CTX_RE = re.compile(r"(æ‡‰åŒ…æ‹¬|æ‡‰è¼‰æ˜|ä¸‹åˆ—(?:å…§å®¹|äº‹é …)?|åŒ…æ‹¬ä¸‹åˆ—|æ‡‰å«|æ‡‰åŒ…å«)")
    ENUM_HEAD_RE = re.compile(r"^\s*(?:[ï¼ˆ(]?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+[ï¼‰)]|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€|\d+[ã€.])\s*")
    
    PAT_ENUM = re.compile(
        r"^(?P<term>[^ï¼š:ï¼Œ,ï¼›;]{1,30})\s*[ï¼š:ï¼Œ,]\s*"
        r"(?:(?:æœ¬æ³•æ‰€ç¨±|æ‰€ç¨±|ä¿‚æŒ‡|æ˜¯æŒ‡|æŒ‡ç¨±|æŒ‡ç‚º|è¬‚ç‚º|è¬‚ä¹‹|æ„æŒ‡|æ„è¬‚|æ„å³|ç¨±ç‚º|ç¨±ä¹‹ç‚º|æ¦‚ç¨±|æ³›æŒ‡)\s*)?"
        r"(?P<def>.+)"
    )
    PAT_SENT_1 = re.compile(
        r"(?:æœ¬æ³•æ‰€ç¨±|æ‰€ç¨±)\s*(?P<term>[^ï¼Œ,ï¼š:ï¼›;]{1,30})[ï¼Œ,ï¼š:]\s*"
        r"(?:ä¿‚æŒ‡|æ˜¯æŒ‡|æŒ‡ç¨±|æŒ‡ç‚º|è¬‚ç‚º|è¬‚ä¹‹|æ„æŒ‡|æ„è¬‚|æ„å³|ç¨±ç‚º|ç¨±ä¹‹ç‚º|æ¦‚ç¨±|æ³›æŒ‡)\s*(?P<def>[^ã€‚ï¼›\n]+)"
    )
    PAT_SENT_2 = re.compile(
        r"^(?P<term>[^ï¼š:ï¼Œ,ï¼›;]{1,30})\s*"
        r"(?:ä¿‚æŒ‡|æ˜¯æŒ‡|æŒ‡ç¨±|æŒ‡ç‚º|è¬‚ç‚º|è¬‚ä¹‹|ç¨±ç‚º|ç¨±ä¹‹ç‚º|æ„æŒ‡|æ„è¬‚|æ„å³)\s*(?P<def>[^ã€‚ï¼›\n]+)"
    )
    
    TERM_SUFFIX = r"(?:ç”¨åœ°|åœ°å€|å€åŸŸ|ç”¨æµ·|ä¿è­·å€|ä¿å®‰å€|é¡åˆ¥|åˆ†å€|å¸¶)"
    SEMANTIC_ENUM_RE = re.compile(
        rf"^(?P<term>[^ï¼š:ï¼Œ,ï¼›;\s]{{1,30}}{TERM_SUFFIX})\s*[ï¼š:]\s*(?P<def>.+)"
    )

patterns = RegexPatterns()

# =============================
# é€šç”¨ AI æœå‹™ç®¡ç†
# =============================

class UniversalAIService:
    """é€šç”¨ AI æœå‹™ç®¡ç†é¡"""
    
    def __init__(self, provider: AIProvider, api_key: str = ""):
        self.provider = provider
        self.api_key = api_key
        self.client: Optional[Union[OpenAI, Any]] = None
        self._initialize_client()
    
    def _initialize_client(self):
        """åˆå§‹åŒ– AI å®¢æˆ¶ç«¯"""
        if self.provider == AIProvider.OLLAMA:
            self.client = OpenAI(
                base_url=f"{config.OLLAMA_BASE_URL}/v1",
                api_key=config.OLLAMA_API_KEY,
                timeout=config.TIMEOUT,
                max_retries=0
            )
        elif self.provider == AIProvider.OPENAI:
            if self.api_key:
                self.client = OpenAI(
                    api_key=self.api_key,
                    timeout=config.TIMEOUT
                )
        elif self.provider == AIProvider.GEMINI:
            if self.api_key and GEMINI_AVAILABLE and genai:
                genai.configure(api_key=self.api_key)  # type: ignore[attr-defined]
                self.client = genai
    
    def get_available_models(self) -> List[str]:
        """å–å¾—å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            if self.provider == AIProvider.OLLAMA:
                r = requests.get(f"{config.OLLAMA_BASE_URL}/api/tags", timeout=5)
                if r.status_code == 200:
                    return [m["name"] for m in r.json().get("models", [])]
            elif self.provider == AIProvider.OPENAI:
                if isinstance(self.client, OpenAI):
                    return [m.id for m in self.client.models.list().data]
            elif self.provider == AIProvider.GEMINI:
                if GEMINI_AVAILABLE and genai is not None:
                    return [
                        m.name for m in genai.list_models()  # type: ignore[attr-defined]
                        if "generateContent" in getattr(m, "supported_generation_methods", [])
                    ]
        except Exception as e:
            st.warning(f"ç„¡æ³•å–å¾—æ¨¡å‹åˆ—è¡¨: {str(e)}")
        return []
    
    def check_service(self) -> bool:
        """æª¢æŸ¥æœå‹™æ˜¯å¦å¯ç”¨"""
        try:
            if self.provider == AIProvider.OLLAMA:
                r = requests.get(f"{config.OLLAMA_BASE_URL}/api/tags", timeout=5)
                return r.status_code == 200
            elif self.provider == AIProvider.OPENAI:
                return bool(self.api_key and isinstance(self.client, OpenAI))
            elif self.provider == AIProvider.GEMINI:
                return bool(self.api_key and GEMINI_AVAILABLE and self.client)
        except Exception:
            pass
        return False
    
    def chat_completion(self, messages: List[Dict[str, str]], model: str, stream: bool = False, **kwargs) -> Any:
        """çµ±ä¸€çš„èŠå¤©å®Œæˆä»‹é¢"""
        if self.provider == AIProvider.OLLAMA or self.provider == AIProvider.OPENAI:
            if not isinstance(self.client, OpenAI):
                raise ValueError("OpenAI client not initialized")
            return self.client.chat.completions.create(
                model=model,
                messages=messages,  # type: ignore
                stream=stream,
                **kwargs
            )
        elif self.provider == AIProvider.GEMINI:
            if not GEMINI_AVAILABLE or not genai:
                raise ValueError("Gemini not available. Install with: pip install google-generativeai")
            
            # è½‰æ›è¨Šæ¯æ ¼å¼çµ¦ Gemini
            gemini_model = genai.GenerativeModel(model)  # type: ignore[attr-defined]
            
            # å°‡ OpenAI æ ¼å¼è½‰æ›ç‚º Gemini æ ¼å¼
            prompt_parts = []
            for msg in messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "system":
                    prompt_parts.append(f"ç³»çµ±æŒ‡ç¤º: {content}\n")
                elif role == "user":
                    prompt_parts.append(f"ä½¿ç”¨è€…: {content}\n")
                elif role == "assistant":
                    prompt_parts.append(f"åŠ©ç†: {content}\n")
            
            full_prompt = "\n".join(prompt_parts)
            
            if stream:
                response = gemini_model.generate_content(full_prompt, stream=True)
                return response
            else:
                response = gemini_model.generate_content(full_prompt)
                return response
        
        return None

# =============================
# æ–‡ä»¶è™•ç†å·¥å…·
# =============================

class FileHandler:
    """æ–‡ä»¶è™•ç†å·¥å…·é¡"""
    
    @staticmethod
    def safe_truncate_text(text: str, max_chars: int = config.MAX_CTX_CHARS) -> str:
        if text is None:
            return ""
        return text if len(text) <= max_chars else text[:max_chars]
    
    @staticmethod
    def load_keywords(path: pathlib.Path) -> List[str]:
        if not path.exists():
            return []
        lines = [l.strip() for l in path.read_text("utf-8").splitlines() if l.strip()]
        return list(dict.fromkeys(lines))

    @staticmethod
    def _decode_bytes(data: bytes) -> str:
        for encoding in ("utf-8-sig", "utf-8", "big5", "cp950"):
            try:
                return data.decode(encoding)
            except UnicodeDecodeError:
                continue
        return data.decode("utf-8", errors="ignore")

    @staticmethod
    def load_uploaded_keywords(file_obj: Optional[Any]) -> List[str]:
        if not file_obj:
            return []
        try:
            text = FileHandler._decode_bytes(file_obj.getvalue())
        except Exception:
            return []
        lines = [l.strip() for l in text.splitlines() if l.strip()]
        return list(dict.fromkeys(lines))
    
    @staticmethod
    def read_txt_files(folder: pathlib.Path, limit: Optional[int] = None):
        """è®€å– TXT æª”æ¡ˆ"""
        if not folder.exists():
            return []
        paths = sorted(folder.glob("*.txt"))
        if limit is not None:
            paths = paths[:limit]
        
        out = []
        for p in paths:
            name = p.stem
            law = name.split("-", 1)[1] if "-" in name else name
            text = p.read_text(encoding="utf-8-sig")
            out.append((law, text, p))
        return out

    @staticmethod
    def read_uploaded_txt_files(files: Optional[List[Any]], limit: Optional[int] = None):
        documents = []
        if not files:
            return documents
        for uploaded in files:
            if not uploaded:
                continue
            name = pathlib.Path(uploaded.name)
            if name.suffix.lower() != ".txt":
                continue
            try:
                text = FileHandler._decode_bytes(uploaded.getvalue())
            except Exception:
                continue
            law_name = name.stem.split("-", 1)[1] if "-" in name.stem else name.stem
            documents.append((law_name, text, None))
            if limit is not None and len(documents) >= limit:
                break
        return documents
    
    @staticmethod
    def get_available_databases() -> Dict[str, pathlib.Path]:
        """å–å¾—å¯ç”¨çš„æ³•è¦è³‡æ–™åº«"""
        available = {}
        for name, path in config.DEFAULT_DATABASES.items():
            if path.exists():
                txt_files = list(path.glob("*.txt"))
                json_files = list(path.glob("*.json"))
                if txt_files or json_files:
                    available[f"{name} ({len(txt_files)} å€‹æª”æ¡ˆ)"] = path
        return available
    
    @staticmethod
    def load_moj_json(dirpath: pathlib.Path) -> List[Dict[str, Any]]:
        """è¼‰å…¥ MOJ JSON æª”æ¡ˆï¼ˆæŒ‰æ³•è¦ä½éšæ’åºï¼šå…ˆ Law å¾Œ Orderï¼‰"""
        out = []
        if not dirpath.exists():
            return out
        
        # å„ªå…ˆè¼‰å…¥æ³•è¦ (ChLaw.json)
        law_file = dirpath / "ChLaw.json"
        if law_file.exists():
            try:
                data = json.loads(law_file.read_text(encoding="utf-8-sig"))
                articles = FileHandler._extract_json_articles(data)
                # æ¨™è¨˜ç‚ºæ³•è¦
                for article in articles:
                    article["source_type"] = "æ³•è¦"
                    article["priority"] = 1
                out.extend(articles)
            except Exception:
                pass
        
        # å…¶æ¬¡è¼‰å…¥å‘½ä»¤ (ChOrder.json)
        order_file = dirpath / "ChOrder.json"
        if order_file.exists():
            try:
                data = json.loads(order_file.read_text(encoding="utf-8-sig"))
                articles = FileHandler._extract_json_articles(data)
                # æ¨™è¨˜ç‚ºå‘½ä»¤
                for article in articles:
                    article["source_type"] = "å‘½ä»¤"
                    article["priority"] = 2
                out.extend(articles)
            except Exception:
                pass
        
        return out

    @staticmethod
    def load_uploaded_json_files(files: Optional[List[Any]]) -> List[Dict[str, Any]]:
        articles: List[Dict[str, Any]] = []
        if not files:
            return articles
        for uploaded in files:
            if not uploaded:
                continue
            try:
                content = FileHandler._decode_bytes(uploaded.getvalue())
                data = json.loads(content)
            except Exception:
                continue
            articles.extend(FileHandler._extract_json_articles(data))
        return articles

    @staticmethod
    def _extract_json_articles(data: Dict[str, Any]) -> List[Dict[str, Any]]:
        results: List[Dict[str, Any]] = []
        laws = data.get("Laws") or data.get("laws") or data.get("items") or []
        for law in laws:
            lawname = (law.get("LawName") or law.get("name") or law.get("Law")
                      or law.get("l") or "").strip()
            # æå–æ³•è¦ä¿®è¨‚æ—¥æœŸ
            law_modified_date = law.get("LawModifiedDate", "")
            arts = (law.get("LawArticles") or law.get("articles")
                    or law.get("Articles") or law.get("cles") or [])
            for a in arts:
                artno = (a.get("ArticleNo") or a.get("no") or a.get("Article")
                         or a.get("icleNo") or "").strip()
                text = (a.get("ArticleContent") or a.get("content")
                        or a.get("ArticleText") or a.get("icleContent") or "").strip()
                if lawname and artno and text:
                    results.append({
                        "law": lawname, 
                        "article": artno, 
                        "text": text,
                        "modified_date": law_modified_date  # åŠ å…¥ä¿®è¨‚æ—¥æœŸ
                    })
        return results

    @staticmethod
    def prepare_documents(source: Dict[str, Any], limit: Optional[int]) -> List[Tuple[str, str, Optional[pathlib.Path]]]:
        if source.get("mode") == "upload":
            return FileHandler.read_uploaded_txt_files(source.get("files"), limit)
        path: Optional[pathlib.Path] = source.get("path")
        if path is None:
            return []
        return FileHandler.read_txt_files(path, limit)

    @staticmethod
    def load_keywords_from_source(source: Dict[str, Any]) -> List[str]:
        if source.get("mode") == "upload":
            return FileHandler.load_uploaded_keywords(source.get("file"))
        return FileHandler.load_keywords(config.KEYWORDS_TXT)

    @staticmethod
    def load_json_articles_from_source(source: Dict[str, Any]) -> List[Dict[str, Any]]:
        if source.get("mode") == "upload":
            return FileHandler.load_uploaded_json_files(source.get("files"))
        path: Optional[pathlib.Path] = source.get("path")
        if path is None:
            return []
        return FileHandler.load_moj_json(path)
    
    @staticmethod
    def _load_term_json(file_path: pathlib.Path, key_name: str) -> Dict[str, str]:
        """é€šç”¨çš„åè©å°ç…§ JSON è¼‰å…¥æ–¹æ³•
        
        Args:
            file_path: JSON æª”æ¡ˆè·¯å¾‘
            key_name: JSON ä¸­çš„ä¸»éµåç¨±ï¼ˆå¦‚ "åŸå½™ç·¨" æˆ– "Taide"ï¼‰
        
        Returns:
            Dict[str, str]: {ä¸­æ–‡åè©: å…§å®¹} çš„å­—å…¸
        """
        term_dict = {}
        if not file_path.exists():
            return term_dict
        
        try:
            data = json.loads(file_path.read_text(encoding="utf-8"))
            term_list = data.get(key_name, [])
            for item in term_list:
                term = item.get("ä¸­æ–‡åè©", "").strip()
                content = item.get("å…§å®¹", "").strip()
                if term and content:
                    term_dict[term] = content
        except Exception as e:
            st.warning(f"è¼‰å…¥ {file_path.name} å¤±æ•—: {str(e)}")
        
        return term_dict
    
    @staticmethod
    def load_origin_json() -> Dict[str, str]:
        """è¼‰å…¥ Origin JSON ä¸¦å»ºç«‹åè©å°ç…§å­—å…¸"""
        return FileHandler._load_term_json(config.ORIGIN_JSON, "åŸå½™ç·¨")
    
    @staticmethod
    def load_taide_json() -> Dict[str, str]:
        """è¼‰å…¥ Taide JSON ä¸¦å»ºç«‹åè©å°ç…§å­—å…¸"""
        return FileHandler._load_term_json(config.TAIDE_JSON, "Taide")

# =============================
# æ³•è¦æ–‡æœ¬åˆ†æå™¨
# =============================

class LegalTextAnalyzer:
    """æ³•è¦æ–‡æœ¬åˆ†æå™¨"""
    
    @staticmethod
    def split_articles(text: str):
        pieces = []
        matches = list(patterns.ARTICLE_HEAD_RE.finditer(text))
        for i, m in enumerate(matches):
            start = m.start()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            pieces.append(("ç¬¬" + m.group(1) + "æ¢", text[start:end].strip()))
        return pieces
    
    @staticmethod
    def parse_enum_block(block: str, allow_semantic: bool):
        out = []
        for seg in re.split(r"[ï¼›;\n]+", block):
            seg = seg.strip()
            if not seg:
                continue
            
            m = patterns.PAT_ENUM.match(seg)
            if m:
                out.append((m.group("term").strip(), m.group("def").strip()))
                continue
            
            if allow_semantic:
                m3 = patterns.SEMANTIC_ENUM_RE.match(seg)
                if m3:
                    out.append((m3.group("term").strip(), m3.group("def").strip()))
                    continue
            
            m1 = patterns.PAT_SENT_1.search(seg) or patterns.PAT_SENT_2.search(seg)
            if m1:
                out.append((m1.group("term").strip(), m1.group("def").strip()))
        return out
    
    @staticmethod
    def extract_candidates(article_text: str):
        cands = []
        head = article_text[:200]
        has_anchor = bool(patterns.ENUM_ANCHOR_RE.search(head))
        has_list_head = bool(patterns.LIST_CTX_RE.search(head))

        lines = [ln.rstrip() for ln in article_text.splitlines()]
        semantic_hint = False
        for ln in lines:
            if patterns.ENUM_HEAD_RE.match(ln):
                content = patterns.ENUM_HEAD_RE.sub("", ln).strip()
                if patterns.SEMANTIC_ENUM_RE.match(content):
                    semantic_hint = True
                    break

        enable_enum = (has_anchor and not has_list_head) or semantic_hint

        if enable_enum:
            buf = []
            for ln in lines:
                if patterns.ENUM_HEAD_RE.match(ln):
                    if buf:
                        cands.extend(LegalTextAnalyzer.parse_enum_block("\n".join(buf), allow_semantic=True))
                        buf = []
                    buf.append(patterns.ENUM_HEAD_RE.sub("", ln))
                else:
                    if buf:
                        buf[-1] += (" " + ln.strip())
            if buf:
                cands.extend(LegalTextAnalyzer.parse_enum_block("\n".join(buf), allow_semantic=True))

        for ln in lines:
            m1 = patterns.PAT_SENT_1.search(ln)
            if m1:
                cands.append((m1.group("term").strip(), m1.group("def").strip()))
            m2 = patterns.PAT_SENT_2.search(ln)
            if m2:
                cands.append((m2.group("term").strip(), m2.group("def").strip()))

        for m in patterns.PAT_SENT_1.finditer(article_text):
            cands.append((m.group("term").strip(), m.group("def").strip()))
        for m in patterns.PAT_SENT_2.finditer(article_text):
            cands.append((m.group("term").strip(), m.group("def").strip()))

        tmp = []
        for t, d in cands:
            if not (1 <= len(t) <= 30):
                continue
            if re.fullmatch(r"[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+", t):
                continue
            tmp.append((t, d.strip().rstrip("ã€‚ï¼›;")))

        cleaned = []
        for t, d in dict.fromkeys(tmp):
            pos = article_text.find(d)
            window = article_text[max(0, pos - 50):pos + len(d) + 50] if pos != -1 else ""
            if window and patterns.LIST_CTX_RE.search(window):
                continue
            cleaned.append((t, d))
        
        return cleaned

# =============================
# LLM è™•ç†å™¨ï¼ˆé€šç”¨ç‰ˆï¼‰
# =============================

class UniversalLLMProcessor:
    """é€šç”¨ LLM è™•ç†å™¨"""
    
    def __init__(self, ai_service: UniversalAIService):
        self.ai_service = ai_service
        self.ollama_options = {
            "num_ctx": 8192,
            "num_batch": 512,
            "flash_attention": True
        }
    
    def validate_term_definition(self, term: str, definition: str, article_text: str, model: str) -> bool:
        """LLM é©—è­‰åè©å®šç¾©"""
        context = FileHandler.safe_truncate_text(article_text, 2000)

        system_prompt = (
            "ä½ æ˜¯å°ç£æ³•å¾‹æ–‡ä»¶åˆ†æåŠ©æ‰‹ã€‚åªèƒ½ä½¿ç”¨ <ä¸Šä¸‹æ–‡> çš„æ–‡å­—åˆ¤æ–·ï¼Œä¸å¾—æ”¹å‹•ä»»ä½•å­—è©ã€‚"
            "ä»»å‹™ï¼šæª¢æŸ¥ candidates æ˜¯å¦ç‚ºã€åè©â€”å®šç¾©ã€ï¼Œå›å‚³åŸæ–‡å­ä¸²ã€‚"
            "è¼¸å‡ºå”¯ä¸€ JSONï¼š{\"results\":[{\"term\":\"\",\"definition\":\"\",\"defined\":true/false}]}"
            "åƒ…æª¢æ ¸èˆ‡å¾®èª¿é‚Šç•Œï¼›ä¸è¦ä»»æ„æ–°å¢ã€‚"
            "åªæœ‰ã€åè©ï¼šâ€¦â€¦ã€æˆ–å«ã€ä¿‚æŒ‡/æŒ‡/è¬‚/ç‚º/ç¨±ç‚º/æ„æŒ‡/æ„å³ã€ï¼Œæˆ–ã€Xç”¨åœ°ï¼šä¾›â€¦ä½¿ç”¨è€…ã€èªç¾©æšèˆ‰ï¼Œä¸”ä¸åœ¨ã€æ‡‰åŒ…æ‹¬/ä¸‹åˆ—å…§å®¹/æ‡‰è¼‰æ˜/æ‡‰åŒ…å«ã€æ¸…å–®èªå¢ƒï¼Œæ‰æ¨™ trueã€‚"
        )
        user_prompt = (
            f"<ä¸Šä¸‹æ–‡>\n{context}\n</ä¸Šä¸‹æ–‡>\n\n"
            f"è«‹æª¢æŸ¥ä¸‹åˆ—å€™é¸æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ã€Œåè©â€”å®šç¾©ã€ï¼š\n"
            f"- åè©ï¼š{term}\n"
            f"- å®šç¾©å€™é¸ï¼š{definition}\n\n"
            "è«‹ä¾æŒ‡ç¤ºè¼¸å‡ºå”¯ä¸€ JSONã€‚"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            if self.ai_service.provider == AIProvider.GEMINI:
                response = self.ai_service.chat_completion(messages, model, stream=False)
                if response and hasattr(response, 'text'):
                    result = response.text
                else:
                    st.warning("é©—è­‰å¤±æ•—ï¼šç„¡æ³•å–å¾— Gemini å›æ‡‰")
                    return True
            else:
                extra_params = {}
                if self.ai_service.provider == AIProvider.OLLAMA:
                    extra_params["extra_body"] = {"options": self.ollama_options}
                
                response = self.ai_service.chat_completion(
                    messages, model,
                    stream=False,
                    timeout=config.TIMEOUT,
                    **extra_params
                )
                if response and hasattr(response, 'choices') and response.choices:
                    result = response.choices[0].message.content or ""
                else:
                    st.warning("é©—è­‰å¤±æ•—ï¼šç„¡æ³•å–å¾—æ¨¡å‹å›æ‡‰")
                    return True
            
            try:
                data = json.loads(result)
                first = data.get("results", [{}])[0]
                return bool(first.get("defined"))
            except Exception:
                st.warning(f"é©—è­‰å¤±æ•—ï¼šå›å‚³æ ¼å¼éŒ¯èª¤ -> {result}")
                return True
        except Exception as e:
            st.warning(f"é©—è­‰å¤±æ•—ï¼š{str(e)}")
            return True
    
    def synthesize_definition(self, term: str, contexts: List[Dict], model: str) -> Tuple[str, str]:
        """ä½¿ç”¨ AI åˆæˆåè©å®šç¾©ï¼Œä¸¦å›å‚³å®šç¾©èˆ‡ä¸Šä¸‹æ–‡æ‘˜è¦"""
        context_texts = "\n\n---\n\n".join([c.get("text", "") for c in contexts[:3]]) if contexts else ""
        truncated_contexts = FileHandler.safe_truncate_text(context_texts, 4000) if context_texts else ""
        
        if truncated_contexts:
            prompt = f"""è«‹ä»”ç´°é–±è®€ä»¥ä¸‹æ³•è¦å…§å®¹ï¼Œé‡å°åè©ã€Œ{term}ã€æ•´ç†å‡ºæ¸…æ¥šçš„å®šç¾©ï¼š

{truncated_contexts}

è«‹æ ¹æ“šä¸Šè¿°æ¢æ–‡ï¼Œçµ¦å‡ºã€Œ{term}ã€çš„å®šç¾©ï¼Œä¸¦ä¿æŒç”¨å­—å°ˆæ¥­ç²¾æº–ã€‚"""
        else:
            prompt = f"""åœ¨æœªæ‰¾åˆ°å°æ‡‰æ¢æ–‡çš„æƒ…æ³ä¸‹ï¼Œè«‹ä¾å°ˆæ¥­çŸ¥è­˜æ¨æ¸¬ã€Œ{term}ã€å¯èƒ½çš„å®šç¾©ï¼Œä¸¦åŠ å…¥ç°¡çŸ­å‰è¨€è¨»æ˜ä¾†æºç‚º AI åˆæˆå»ºè­°ã€‚"""
        
        messages = [{"role": "user", "content": prompt}]
        
        try:
            if self.ai_service.provider == AIProvider.GEMINI:
                response = self.ai_service.chat_completion(messages, model, stream=False)
                if response and hasattr(response, 'text'):
                    definition = response.text
                else:
                    definition = "ï¼ˆAI ç„¡æ³•ç”Ÿæˆå®šç¾©ï¼‰"
            else:
                extra_params = {}
                if self.ai_service.provider == AIProvider.OLLAMA:
                    extra_params["extra_body"] = {"options": self.ollama_options}
                
                response = self.ai_service.chat_completion(
                    messages, model,
                    stream=False,
                    timeout=config.TIMEOUT,
                    **extra_params
                )
                if response and hasattr(response, 'choices') and response.choices:
                    definition = response.choices[0].message.content or "ï¼ˆAI ç„¡æ³•ç”Ÿæˆå®šç¾©ï¼‰"
                else:
                    definition = "ï¼ˆAI ç„¡æ³•ç”Ÿæˆå®šç¾©ï¼‰"
        except Exception as e:
            definition = f"ï¼ˆAI ç”Ÿæˆå¤±æ•—: {str(e)}ï¼‰"
        
        context_summary = truncated_contexts if truncated_contexts else "AI åˆæˆå»ºè­°ï¼šæœªåœ¨è³‡æ–™åº«æ‰¾åˆ°å°æ‡‰æ¢æ–‡"
        return definition, context_summary

# =============================
# ä¸‹è¼‰è™•ç†å™¨
# =============================

class DownloadHandler:
    """ä¸‹è¼‰è™•ç†å·¥å…·é¡"""
    
    @staticmethod
    def to_downloadable_excel(df: pd.DataFrame, filename: str = "analysis_results.xlsx"):
        buf = io.BytesIO()
        try:
            with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
                df.to_excel(writer, index=False, sheet_name='åˆ†æçµæœ')
                workbook = writer.book
                if hasattr(workbook, "set_properties"):
                    workbook.set_properties({'title': filename})  # type: ignore[attr-defined]
                
                worksheet = writer.sheets['åˆ†æçµæœ']
                for i, col in enumerate(df.columns):
                    max_length = max(
                        df[col].astype(str).map(len).max(),
                        len(str(col))
                    )
                    worksheet.set_column(i, i, min(max_length + 2, 50))
            
            buf.seek(0)
            return buf
        except Exception as e:
            st.error(f"ç”¢ç”ŸExcelæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
    
    @staticmethod
    def to_downloadable_csv(df: pd.DataFrame, filename: str = "analysis_results.csv"):
        try:
            csv_string = df.to_csv(index=False, encoding='utf-8-sig')
            return csv_string.encode('utf-8-sig')
        except Exception as e:
            st.error(f"ç”¢ç”ŸCSVæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None

    @staticmethod
    def build_export_filenames(prefix: str, model: str) -> Tuple[str, str]:
        safe_model_name = model.replace(':', '_').replace('/', '_').replace('\\', '_')
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        base = f"{prefix}_{safe_model_name}_{timestamp}"
        return f"{base}.xlsx", f"{base}.csv"

# =============================
# UI é¡åˆ¥
# =============================

class StreamlitUI:
    """Streamlit UI å·¥å…·é¡"""
    
    @staticmethod
    def setup_page():
        st.set_page_config(
            page_title="æ³•è¦åè©å®šç¾©åˆ†æç³»çµ±",
            page_icon="âš–ï¸",
            layout="wide"
        )
        st.title("âš–ï¸ æ³•è¦åè©å®šç¾©åˆ†æç³»çµ±")
        st.caption("æ”¯æ´ Ollamaã€OpenAIã€Gemini | æœ¬åœ°è³‡æ–™åº«èˆ‡æª”æ¡ˆä¸Šå‚³")
    
    @staticmethod
    def render_sidebar():
        """æ¸²æŸ“å´é‚Šæ¬„ - è¿”å› (ai_service, model, mode, dataset_source, keyword_source, use_stream)"""
        with st.sidebar:
            st.header("ğŸ› ï¸ ç³»çµ±è¨­å®š")
            if "openai_api_key" not in st.session_state:
                st.session_state["openai_api_key"] = config.OPENAI_API_KEY
            if "gemini_api_key" not in st.session_state:
                st.session_state["gemini_api_key"] = config.GEMINI_API_KEY
            
            # AI æœå‹™é¸æ“‡
            st.subheader("ğŸ¤– AI æœå‹™")
            
            # æª¢æŸ¥ Gemini æ˜¯å¦å¯ç”¨
            available_providers = [AIProvider.OLLAMA.value, AIProvider.OPENAI.value]
            if GEMINI_AVAILABLE:
                available_providers.append(AIProvider.GEMINI.value)
            else:
                st.caption("âš ï¸ Gemini ä¸å¯ç”¨ï¼ˆéœ€å®‰è£ google-generativeaiï¼‰")
            
            provider_choice = st.selectbox(
                "é¸æ“‡ AI æœå‹™",
                options=available_providers,
                index=0
            )
            
            # API Key è¼¸å…¥
            api_key = ""
            if provider_choice == AIProvider.OPENAI.value:
                stored_key = st.session_state.get("openai_api_key", config.OPENAI_API_KEY)
                if not isinstance(stored_key, str):
                    stored_key = config.OPENAI_API_KEY
                api_key = st.text_input(
                    "OpenAI API Key",
                    type="password",
                    value=stored_key,
                    help="è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key"
                )
                if api_key != stored_key:
                    st.session_state["openai_api_key"] = api_key
            elif provider_choice == AIProvider.GEMINI.value:
                stored_key = st.session_state.get("gemini_api_key", config.GEMINI_API_KEY)
                if not isinstance(stored_key, str):
                    stored_key = config.GEMINI_API_KEY
                api_key = st.text_input(
                    "Gemini API Key",
                    type="password",
                    value=stored_key,
                    help="è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key"
                )
                if api_key != stored_key:
                    st.session_state["gemini_api_key"] = api_key
            
            # å»ºç«‹ AI æœå‹™
            provider_enum = AIProvider.OLLAMA
            for p in AIProvider:
                if p.value == provider_choice:
                    provider_enum = p
                    break
            
            ai_service = UniversalAIService(provider_enum, api_key)
            
            # æª¢æŸ¥æœå‹™ç‹€æ…‹
            service_ok = ai_service.check_service()
            if service_ok:
                st.success(f"âœ… {provider_choice} æœå‹™æ­£å¸¸")
            else:
                st.error(f"âŒ {provider_choice} æœå‹™ç„¡æ³•é€£æ¥")
                if provider_enum == AIProvider.OLLAMA:
                    st.info("è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œ")
                else:
                    st.info("è«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢º")
            
            # æ¨¡å‹é¸æ“‡
            st.subheader("ğŸ¯ æ¨¡å‹é¸æ“‡")
            available_models = ai_service.get_available_models()
            
            if available_models:
                selected_model = st.selectbox(
                    "é¸æ“‡æ¨¡å‹",
                    options=available_models,
                    help=f"å¾ {provider_choice} é¸æ“‡å¯ç”¨çš„æ¨¡å‹"
                )
            else:
                st.warning(f"âš ï¸ ç„¡æ³•å–å¾— {provider_choice} æ¨¡å‹åˆ—è¡¨")
                selected_model = st.text_input("æ‰‹å‹•è¼¸å…¥æ¨¡å‹åç¨±", value="gpt-3.5-turbo")
            
            # æ³•è¦è³‡æ–™åº«é¸æ“‡
            st.subheader("ğŸ“š æ³•è¦è³‡æ–™åº«")
            database_mode = st.radio(
                "è³‡æ–™ä¾†æº",
                options=["ä½¿ç”¨æœ¬åœ°è³‡æ–™å¤¾", "ä¸Šå‚³ TXT æª”"],
                key="database_mode"
            )

            dataset_source: Dict[str, Any]

            if database_mode == "ä½¿ç”¨æœ¬åœ°è³‡æ–™å¤¾":
                available_dbs = FileHandler.get_available_databases()
                if available_dbs:
                    selected_db_name = st.selectbox(
                        "é¸æ“‡æ³•è¦è³‡æ–™åº«",
                        options=list(available_dbs.keys()),
                        help="é¸æ“‡è¦åˆ†æçš„æ³•è¦è³‡æ–™åº«"
                    )
                    database_path = available_dbs[selected_db_name]
                    txt_files = list(database_path.glob("*.txt"))
                    st.info(f"ğŸ“ è³‡æ–™å¤¾: {database_path.name}\n\nğŸ“„ æª”æ¡ˆæ•¸: {len(txt_files)}")
                else:
                    st.error("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„æ³•è¦è³‡æ–™åº«")
                    database_path = config.DEFAULT_DATABASES.get("åœ‹åœŸè¨ˆç•«æ³•è¦", config.BASE_DIR)
                dataset_source = {"mode": "local", "path": database_path}
            else:
                uploaded_files = st.file_uploader(
                    "ä¸Šå‚³ä¸€å€‹æˆ–å¤šå€‹ TXT æª”æ¡ˆ",
                    type=["txt"],
                    accept_multiple_files=True,
                    key="uploaded_db_files"
                )
                file_count = len(uploaded_files or [])
                st.info(f"ğŸ“„ å·²é¸æ“‡ {file_count} å€‹æª”æ¡ˆ")
                dataset_source = {"mode": "upload", "files": uploaded_files}

            # é—œéµå­—ä¾†æº
            st.subheader("ğŸ”‘ é—œéµå­—åˆ—è¡¨")
            keyword_mode = st.radio(
                "é—œéµå­—ä¾†æº",
                options=["ä½¿ç”¨é è¨­æª”æ¡ˆ", "ä¸Šå‚³ TXT"],
                key="keyword_mode"
            )

            keyword_source: Dict[str, Any]
            if keyword_mode == "ä¸Šå‚³ TXT":
                keyword_file = st.file_uploader(
                    "ä¸Šå‚³é—œéµå­— TXT æª”æ¡ˆ",
                    type=["txt"],
                    key="keyword_upload"
                )
                keyword_source = {"mode": "upload", "file": keyword_file}
                if keyword_file:
                    st.caption(f"ğŸ”¤ é—œéµå­—æª”æ¡ˆï¼š{keyword_file.name}")
            else:
                keyword_source = {"mode": "local"}
                st.caption(f"ä½¿ç”¨é è¨­æª”æ¡ˆï¼š{config.KEYWORDS_TXT.name}")
            
            # æ¨¡å¼é¸æ“‡
            st.subheader("âš™ï¸ åˆ†ææ¨¡å¼")
            mode = st.radio(
                "é¸æ“‡æ¨¡å¼",
                options=["æ³•è¦åˆ†æ", "AI èŠå¤©"],
                index=0
            )
            
            # ä¸²æµæ¨¡å¼è¨­å®šï¼ˆåƒ…åœ¨ AI èŠå¤©æ¨¡å¼é¡¯ç¤ºï¼‰
            use_stream = True
            if mode == "AI èŠå¤©":
                st.subheader("ğŸ”§ èŠå¤©è¨­å®š")
                use_stream = st.checkbox(
                    "å•Ÿç”¨ä¸²æµæ¨¡å¼",
                    value=True,
                    help="ä¸²æµæ¨¡å¼ï¼šé€å­—é¡¯ç¤ºå›æ‡‰ï¼ˆéœ€çµ„ç¹”é©—è­‰ï¼‰\néä¸²æµæ¨¡å¼ï¼šä¸€æ¬¡é¡¯ç¤ºå®Œæ•´å›æ‡‰"
                )
                if not use_stream:
                    st.info("ğŸ’¡ ä½¿ç”¨éä¸²æµæ¨¡å¼ï¼ˆé©ç”¨æ–¼æœªé©—è­‰çš„ OpenAI çµ„ç¹”ï¼‰")
            
            st.divider()
            st.caption("ğŸ’¡ æç¤ºï¼š")
            st.caption("â€¢ Ollamaï¼šåœ°ç«¯ï¼Œå…è²»")
            st.caption("â€¢ OpenAIï¼šéœ€è¦ API Key")
            st.caption("â€¢ Geminiï¼šéœ€è¦ API Key")
            if provider_choice == AIProvider.OPENAI.value:
                st.caption("â€¢ ä¸²æµæ¨¡å¼éœ€çµ„ç¹”é©—è­‰")
        
        return ai_service, selected_model, mode, dataset_source, keyword_source, use_stream

    @staticmethod
    def display_results(rows: List[Dict]) -> Optional[pd.DataFrame]:
        """é¡¯ç¤ºåˆ†æçµæœ"""
        if not rows:
            st.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•åè©å®šç¾©")
            return None
        
        # çµ±è¨ˆè³‡è¨Š
        st.subheader("ğŸ“Š åˆ†æçµæœçµ±è¨ˆ")
        df = pd.DataFrame(rows)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç¸½è¨ˆåè©", len(df))
        with col2:
            keyword_count = len(df[df["ä¸»é¡Œè©"] == "æ˜¯"]) if "ä¸»é¡Œè©" in df.columns else 0
            st.metric("ä¸»é¡Œè©", keyword_count)
        with col3:
            has_def_count = len(df[df["æœ‰ç„¡å®šç¾©"] == "æœ‰"]) if "æœ‰ç„¡å®šç¾©" in df.columns else len(df)
            st.metric("æœ‰å®šç¾©", has_def_count)
        with col4:
            unique_sources = df["å®šç¾©ä¾†æº"].nunique() if "å®šç¾©ä¾†æº" in df.columns else 1
            st.metric("ä¾†æºæ•¸", unique_sources)
        
        # é¡¯ç¤ºçµæœè¡¨æ ¼
        st.subheader("ğŸ“‹ è©³ç´°çµæœ")
        st.dataframe(df, use_container_width=True, height=400)
        
        # åˆ†çµ„çµ±è¨ˆ
        if "å®šç¾©ä¾†æº" in df.columns:
            st.subheader("ğŸ“ˆ ä¾†æºåˆ†ä½ˆ")
            source_counts = df["å®šç¾©ä¾†æº"].value_counts()
            col1, col2 = st.columns(2)
            with col1:
                st.bar_chart(source_counts)
            with col2:
                for source, count in source_counts.items():
                    st.write(f"**{source}**: {count} å€‹")
        
        return df

# =============================
# æ³•è¦åˆ†æå¼•æ“
# =============================

class LegalAnalysisEngine:
    """æ³•è¦åˆ†æå¼•æ“"""
    
    def __init__(self, llm_processor: UniversalLLMProcessor):
        self.llm = llm_processor
        self.analyzer = LegalTextAnalyzer()
        # è¼‰å…¥ Origin JSON å­—å…¸
        self.origin_dict = FileHandler.load_origin_json()
        # è¼‰å…¥ Taide JSON å­—å…¸
        self.taide_dict = FileHandler.load_taide_json()
    
    def check_origin_term(self, term: str) -> Tuple[str, str]:
        """æª¢æŸ¥åè©æ˜¯å¦å­˜åœ¨æ–¼ Origin JSON ä¸­
        
        Returns:
            Tuple[str, str]: (æ˜¯å¦å­˜åœ¨("æ˜¯"/"å¦"), åŸå½™ç·¨å®šç¾©å…§å®¹)
        """
        if term in self.origin_dict:
            return "æ˜¯", self.origin_dict[term]
        return "å¦", ""
    
    def check_taide_term(self, term: str) -> Tuple[str, str]:
        """æª¢æŸ¥åè©æ˜¯å¦å­˜åœ¨æ–¼ Taide JSON ä¸­
        
        Returns:
            Tuple[str, str]: (æ˜¯å¦å­˜åœ¨("æ˜¯"/"å¦"), Taideå®šç¾©å…§å®¹)
        """
        if term in self.taide_dict:
            return "æ˜¯", self.taide_dict[term]
        return "å¦", ""
    
    def analyze_full(
        self,
        dataset_source: Dict[str, Any],
        keyword_source: Dict[str, Any],
        limit_files: Optional[int],
        use_llm_validation: bool,
        include_json_search: bool,
        model: str,
    ) -> List[Dict]:
        """å®Œæ•´åˆ†æï¼ˆåŸç‰ˆé‚è¼¯ï¼‰"""
        txt_items = FileHandler.prepare_documents(dataset_source, limit_files)

        keywords = set(FileHandler.load_keywords_from_source(keyword_source))

        json_articles: List[Dict[str, Any]] = []
        if include_json_search:
            json_source: Dict[str, Any] = {
                "mode": "local",
                "path": config.DEFAULT_DATABASES.get("å…¨åœ‹æ³•è¦JSON"),
            }
            json_articles = FileHandler.load_json_articles_from_source(json_source)
        
        if not txt_items:
            st.warning("æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ³•è¦æª”æ¡ˆ")
            return []
        
        rows = []
        extracted_terms = set()
        
        total_steps = len(txt_items) + (len(keywords) if include_json_search else 0)
        current_step = 0
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # A) å¾ TXT æŠ½å–
        status_text.text("ğŸ“„ åˆ†æ TXT æ³•è¦æª”æ¡ˆ...")
        for law, full_text, _ in txt_items:
            status_text.text(f"æ­£åœ¨åˆ†æ: {law}")
            
            articles = self.analyzer.split_articles(full_text)
            
            for art_no, art_txt in articles:
                # æª¢æŸ¥æ˜¯å¦æœ‰å®šç¾©ä¿¡è™Ÿ
                if not patterns.SIGNALS_RE.search(art_txt):
                    continue
                
                # æŠ½å–å€™é¸åè©
                candidates = self.analyzer.extract_candidates(art_txt)
                
                # LLM é©—è­‰ï¼ˆå¯é¸ï¼‰
                if use_llm_validation:
                    validated_results = []
                    for term, definition in candidates:
                        if self.llm.validate_term_definition(term, definition, art_txt, model):
                            validated_results.append({"term": term, "definition": definition})
                    results = validated_results
                else:
                    results = [{"term": t, "definition": d} for t, d in candidates]
                
                # è¨˜éŒ„çµæœ
                for r in results:
                    term = r["term"]
                    extracted_terms.add(term)
                    
                    # æª¢æŸ¥æ˜¯å¦å­˜åœ¨æ–¼ Origin JSON
                    origin_exists, origin_content = self.check_origin_term(term)
                    
                    # æª¢æŸ¥æ˜¯å¦å­˜åœ¨æ–¼ Taide JSON
                    taide_exists, taide_content = self.check_taide_term(term)
                    
                    rows.append({
                        "åè©": term,
                        "ä¸»é¡Œè©": "æ˜¯" if term in keywords else "å¦",
                        "æœ‰ç„¡å®šç¾©": "æœ‰",
                        "å®šç¾©ä¾†æº": config.TXT_SOURCE_LABEL,
                        "æ³•è¦ä¾†æº": f"{law} {art_no}",
                        "å®šç¾©": r["definition"],
                        "ä¾†æºä¾æ“š(ä¸Šä¸‹æ–‡)": art_txt,
                        "åŸå½™ç·¨è©": origin_exists,
                        "åŸå½™ç·¨å®šç¾©": origin_content,
                        "Taideè©": taide_exists,
                        "Taideå®šç¾©": taide_content
                    })
            
            current_step += 1
            progress_bar.progress(min(1.0, current_step / max(1, total_steps)))
        
        # B) ä¸»é¡Œå­—è£œæŸ¥ï¼ˆå¾ JSON è³‡æ–™åº«ï¼‰
        if include_json_search and keywords and json_articles:
            status_text.text("ğŸ” ä¸»é¡Œå­—è£œæŸ¥...")
            for kw in sorted(keywords):
                if kw in extracted_terms:
                    continue
                
                status_text.text(f"è£œæŸ¥ä¸»é¡Œå­—: {kw}")
                rec = self._search_json_for_definition(kw, json_articles, model)
                
                # åˆ¤æ–·å®šç¾©æ˜¯å¦æœ‰æ•ˆï¼š
                # 1. å®šç¾©å…§å®¹å­˜åœ¨ä¸”ä¸æ˜¯éŒ¯èª¤è¨Šæ¯
                # 2. å¿…é ˆæœ‰ä¸Šä¸‹æ–‡ï¼ˆå¾ JSON è³‡æ–™åº«æ‰¾åˆ°ç›¸é—œæ¢æ–‡ï¼‰
                definition_content = rec.get("å®šç¾©", "")
                has_context = rec.get("has_context", True)  # å¾ JSON æŠ½å–çš„ä¸€å®šæœ‰ä¸Šä¸‹æ–‡
                has_valid_definition = (
                    bool(definition_content and not definition_content.startswith("âŒ"))
                    and has_context  # å¿…é ˆæœ‰ä¸Šä¸‹æ–‡æ‰ç®—æœ‰å®šç¾©
                )
                
                # æª¢æŸ¥æ˜¯å¦å­˜åœ¨æ–¼ Origin JSON
                origin_exists, origin_content = self.check_origin_term(kw)
                
                # æª¢æŸ¥æ˜¯å¦å­˜åœ¨æ–¼ Taide JSON
                taide_exists, taide_content = self.check_taide_term(kw)
                
                rows.append({
                    "åè©": rec["åè©"],
                    "ä¸»é¡Œè©": "æ˜¯",
                    "æœ‰ç„¡å®šç¾©": "æœ‰" if has_valid_definition else "ç„¡",
                    "å®šç¾©ä¾†æº": rec.get("å®šç¾©ä¾†æº", config.AI_SOURCE_LABEL),
                    "æ³•è¦ä¾†æº": rec.get("æ³•è¦ä¾†æº", ""),
                    "å®šç¾©": definition_content,
                    "ä¾†æºä¾æ“š(ä¸Šä¸‹æ–‡)": rec.get("ä¾†æºä¾æ“š(ä¸Šä¸‹æ–‡)", ""),
                    "åŸå½™ç·¨è©": origin_exists,
                    "åŸå½™ç·¨å®šç¾©": origin_content,
                    "Taideè©": taide_exists,
                    "Taideå®šç¾©": taide_content
                })
                
                current_step += 1
                progress_bar.progress(min(1.0, current_step / max(1, total_steps)))
        
        status_text.text("âœ… åˆ†æå®Œæˆï¼")
        return rows
    
    def _search_json_for_definition(self, term: str, json_articles: List[Dict], model: str) -> Dict:
        """åœ¨ JSON è³‡æ–™åº«ä¸­æœå°‹åè©å®šç¾©ï¼ˆå„ªå…ˆæ³•è¦ï¼Œæ¬¡ä¹‹å‘½ä»¤ï¼›åŒä½éšå‰‡å„ªå…ˆæ–°æ³•ï¼‰ã€‚æ‰¾ä¸åˆ°å‰‡ç”± AI åˆæˆã€‚"""
        found_contexts_law = []  # æ³•è¦ä¾†æº
        found_contexts_order = []  # å‘½ä»¤ä¾†æº
        
        # æœå°‹åŒ…å«è©²åè©çš„æ¢æ–‡ï¼Œä¸¦æŒ‰ä¾†æºé¡å‹åˆ†é¡
        for article in json_articles:
            text = article.get("text", "")
            if term in text and patterns.SIGNALS_RE.search(text):
                context = {
                    "law": article.get("law", ""),
                    "article": article.get("article", ""),
                    "text": text,
                    "source_type": article.get("source_type", "æœªçŸ¥"),
                    "priority": article.get("priority", 99),
                    "modified_date": article.get("modified_date", "")  # åŠ å…¥ä¿®è¨‚æ—¥æœŸ
                }
                
                # æ ¹æ“šä¾†æºé¡å‹åˆ†é¡
                if article.get("priority", 99) == 1:  # æ³•è¦
                    found_contexts_law.append(context)
                else:  # å‘½ä»¤æˆ–å…¶ä»–
                    found_contexts_order.append(context)
        
        # åœ¨å„è‡ªé¡åˆ¥å…§ï¼ŒæŒ‰ä¿®è¨‚æ—¥æœŸæ’åºï¼ˆæ–°çš„åœ¨å‰ï¼‰
        found_contexts_law.sort(key=lambda x: x.get("modified_date", ""), reverse=True)
        found_contexts_order.sort(key=lambda x: x.get("modified_date", ""), reverse=True)
        
        # å„ªå…ˆä½¿ç”¨æ³•è¦çš„å®šç¾©
        found_contexts = found_contexts_law if found_contexts_law else found_contexts_order
        
        # å¦‚æœæ‰¾åˆ°ï¼Œæå–å®šç¾©
        if found_contexts:
            best_match = found_contexts[0]
            candidates = self.analyzer.extract_candidates(best_match["text"])
            
            for t, d in candidates:
                if t == term:
                    modified_date = best_match.get("modified_date", "")
                    date_display = f" ({modified_date})" if modified_date else ""
                    source_label = f"ã€{best_match.get('source_type', 'æœªçŸ¥')}ã€‘{best_match['law']} {best_match['article']}{date_display}"
                    return {
                        "åè©": term,
                        "å®šç¾©ä¾†æº": config.JSON_SOURCE_LABEL,
                        "æ³•è¦ä¾†æº": source_label,
                        "å®šç¾©": d,
                        "ä¾†æºä¾æ“š(ä¸Šä¸‹æ–‡)": "\n\n---\n\n".join([c["text"] for c in found_contexts[:3]]),
                        "has_context": True  # å¾ JSON æ‰¾åˆ°çš„ä¸€å®šæœ‰ä¸Šä¸‹æ–‡
                    }
        
        # è‹¥æœªæ‰¾åˆ°ç¬¦åˆå®šç¾©çš„æ¢æ–‡ï¼Œå˜—è©¦æ”¾å¯¬æ¢ä»¶æ”¶é›†ä¸Šä¸‹æ–‡ï¼ˆä»å„ªå…ˆæ³•è¦ï¼‰
        if not found_contexts:
            relaxed_law = []
            relaxed_order = []
            
            for article in json_articles:
                text = article.get("text", "")
                if term in text:
                    context = {
                        "law": article.get("law", ""),
                        "article": article.get("article", ""),
                        "text": text,
                        "source_type": article.get("source_type", "æœªçŸ¥"),
                        "priority": article.get("priority", 99),
                        "modified_date": article.get("modified_date", "")
                    }
                    
                    if article.get("priority", 99) == 1:
                        relaxed_law.append(context)
                    else:
                        relaxed_order.append(context)
            
            # å„é¡åˆ¥å…§æŒ‰ä¿®è¨‚æ—¥æœŸæ’åºï¼ˆç”±æ–°åˆ°èˆŠï¼‰
            relaxed_law.sort(key=lambda x: x.get("modified_date", ""), reverse=True)
            relaxed_order.sort(key=lambda x: x.get("modified_date", ""), reverse=True)
            
            # å„ªå…ˆä½¿ç”¨æ³•è¦çš„ä¸Šä¸‹æ–‡ï¼Œå†ä½¿ç”¨å‘½ä»¤
            found_contexts = (relaxed_law + relaxed_order)[:3]
        
        # æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ AI åˆæˆ
        synth_def, synth_context = self.llm.synthesize_definition(term, found_contexts, model)
        suggested_source = "AI åˆæˆå»ºè­°"
        if found_contexts:
            first_ctx = found_contexts[0]
            source_type = first_ctx.get('source_type', 'æœªçŸ¥')
            suggested_source = f"ã€{source_type}ã€‘{first_ctx.get('law', '')} {first_ctx.get('article', '')}".strip()
        
        return {
            "åè©": term,
            "å®šç¾©ä¾†æº": config.AI_SOURCE_LABEL,
            "æ³•è¦ä¾†æº": suggested_source,
            "å®šç¾©": synth_def,
            "ä¾†æºä¾æ“š(ä¸Šä¸‹æ–‡)": synth_context,
            "has_context": bool(found_contexts)  # æ¨™è¨˜æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡
        }

# =============================
# ä¸»ç¨‹å¼
# =============================

def main():
    """ä¸»ç¨‹å¼"""
    StreamlitUI.setup_page()
    (
        ai_service,
        selected_model,
        mode,
        dataset_source,
        keyword_source,
        use_stream,
    ) = StreamlitUI.render_sidebar()
    
    llm_processor = UniversalLLMProcessor(ai_service)
    analysis_engine = LegalAnalysisEngine(llm_processor)

    def build_dataset_info(source: Dict[str, Any]) -> Dict[str, Any]:
        info: Dict[str, Any] = {
            "mode": source.get("mode", "local"),
            "has_documents": False,
            "count": 0,
        }
        if info["mode"] == "upload":
            files = source.get("files") or []
            info["files"] = files
            info["count"] = len(files)
            info["has_documents"] = len(files) > 0
            if files:
                preview = ", ".join(f.name for f in files[:3])
                if len(files) > 3:
                    preview += " ..."
                info["preview"] = preview
            return info

        path = source.get("path")
        info["path"] = path
        if isinstance(path, pathlib.Path) and path.exists():
            txt_files = list(path.glob("*.txt"))
            info["txt_files"] = txt_files
            info["count"] = len(txt_files)
            info["has_documents"] = len(txt_files) > 0
        return info

    dataset_info = build_dataset_info(dataset_source)
    
    if mode == "æ³•è¦åˆ†æ":
        st.header("ğŸ“‹ æ³•è¦åè©å®šç¾©æŠ½å–")
        
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ğŸ“ è³‡æ–™åº«ç‹€æ…‹")
            if dataset_info["mode"] == "upload":
                files = dataset_info.get("files", [])
                st.write("è³‡æ–™ä¾†æºï¼šä¸Šå‚³ TXT æª”æ¡ˆ")
                st.write(f"ğŸ“„ ä¸Šå‚³æª”æ¡ˆæ•¸: {dataset_info.get('count', 0)}")
                if dataset_info.get("preview"):
                    st.caption(dataset_info["preview"])
            else:
                st.write("è³‡æ–™ä¾†æºï¼šæœ¬åœ°è³‡æ–™å¤¾")
                st.write(f"ğŸ“„ æ³•è¦è³‡æ–™åº«: {'âœ…' if dataset_info.get('has_documents') else 'âŒ'}")
                if dataset_info.get("has_documents"):
                    st.write(f"æ‰¾åˆ° {dataset_info.get('count', 0)} å€‹æ³•è¦æª”æ¡ˆ")
                path = dataset_info.get("path")
                if isinstance(path, pathlib.Path):
                    st.info(f"è³‡æ–™å¤¾: {path}")
                else:
                    st.info("æœªæŒ‡å®šè³‡æ–™å¤¾")

        has_documents = bool(dataset_info.get("has_documents"))
        
        with col2:
            st.subheader("âš™ï¸ åˆ†æè¨­å®š")
            limit_files = st.number_input("é™åˆ¶è™•ç†æª”æ¡ˆæ•¸é‡ (0=å…¨éƒ¨)", min_value=0, value=3)
            if limit_files == 0:
                limit_files = None
            
            use_llm_validation = st.checkbox("ä½¿ç”¨LLMé©—è­‰", value=True)
            include_json_search = st.checkbox("å•Ÿç”¨ä¸»é¡Œå­—è£œæŸ¥", value=False, 
                                             help="å¾ mojLawSplitJSON è£œæŸ¥æœªæ‰¾åˆ°çš„ä¸»é¡Œå­—")
            
            if st.button("ğŸš€ é–‹å§‹å®Œæ•´åˆ†æ", type="primary", use_container_width=True):
                if not has_documents:
                    st.error("è«‹æä¾›è‡³å°‘ä¸€å€‹æ³•è¦ TXT æª”æ¡ˆ")
                else:
                    st.info("ğŸ”„ é–‹å§‹å®Œæ•´æ³•è¦åˆ†æ...")
                    
                    rows = analysis_engine.analyze_full(
                        dataset_source,
                        keyword_source,
                        limit_files,
                        use_llm_validation,
                        include_json_search,
                        selected_model,
                    )
                    
                    df = StreamlitUI.display_results(rows)
                    
                    if df is not None:
                        st.subheader("ğŸ“¥ ä¸‹è¼‰çµæœ")
                        col_download1, col_download2 = st.columns(2)
                        
                        excel_filename, csv_filename = DownloadHandler.build_export_filenames("lawhits", selected_model)

                        with col_download1:
                            excel_buf = DownloadHandler.to_downloadable_excel(df, excel_filename)
                            if excel_buf:
                                st.download_button(
                                    label="ğŸ“Š ä¸‹è¼‰ Excel æª”æ¡ˆ",
                                    data=excel_buf,
                                    file_name=excel_filename,
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                    use_container_width=True
                                )

                        with col_download2:
                            csv_data = DownloadHandler.to_downloadable_csv(df, csv_filename)
                            if csv_data:
                                st.download_button(
                                    label="ğŸ“‹ ä¸‹è¼‰ CSV æª”æ¡ˆ",
                                    data=csv_data,
                                    file_name=csv_filename,
                                    mime="text/csv",
                                    use_container_width=True
                                )
    
    else:
        # AI èŠå¤©æ¨¡å¼
        st.header("ğŸ’¬ AI èŠå¤©å°è©±")
        
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            with st.chat_message("user"):
                st.markdown(prompt)
            
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                try:
                    if ai_service.provider == AIProvider.GEMINI:
                        # Gemini ä¸²æµè™•ç†
                        if use_stream:
                            response = ai_service.chat_completion(
                                st.session_state.messages,
                                selected_model,
                                stream=True
                            )
                            
                            if response:
                                for chunk in response:
                                    if hasattr(chunk, 'text'):
                                        full_response += chunk.text
                                        message_placeholder.markdown(full_response + "â–Œ")
                        else:
                            response = ai_service.chat_completion(
                                st.session_state.messages,
                                selected_model,
                                stream=False
                            )
                            if response and hasattr(response, 'text'):
                                full_response = response.text
                                message_placeholder.markdown(full_response)
                    else:
                        # OpenAI æˆ– Ollama
                        extra_params = {}
                        if ai_service.provider == AIProvider.OLLAMA:
                            extra_params["extra_body"] = {"options": llm_processor.ollama_options}
                        
                        if use_stream:
                            # ä½¿ç”¨ä¸²æµæ¨¡å¼
                            try:
                                response = ai_service.chat_completion(
                                    st.session_state.messages,
                                    selected_model,
                                    stream=True,
                                    **extra_params
                                )
                                
                                if response:
                                    for chunk in response:
                                        if hasattr(chunk, 'choices') and chunk.choices and chunk.choices[0].delta.content is not None:
                                            full_response += chunk.choices[0].delta.content
                                            message_placeholder.markdown(full_response + "â–Œ")
                            
                            except Exception as stream_error:
                                # å¦‚æœä¸²æµå¤±æ•—ï¼ˆå¦‚çµ„ç¹”æœªé©—è­‰ï¼‰ï¼Œè‡ªå‹•é™ç´šç‚ºéä¸²æµæ¨¡å¼
                                if "stream" in str(stream_error).lower() or "unsupported_value" in str(stream_error).lower():
                                    message_placeholder.markdown("âš ï¸ ä¸²æµæ¨¡å¼ä¸å¯ç”¨ï¼Œè‡ªå‹•åˆ‡æ›ç‚ºæ¨™æº–æ¨¡å¼...\n\n")
                                    response = ai_service.chat_completion(
                                        st.session_state.messages,
                                        selected_model,
                                        stream=False,
                                        **extra_params
                                    )
                                    if response and hasattr(response, 'choices') and response.choices:
                                        full_response = response.choices[0].message.content or ""
                                        message_placeholder.markdown(full_response)
                                else:
                                    raise stream_error
                        else:
                            # ä½¿ç”¨éä¸²æµæ¨¡å¼
                            response = ai_service.chat_completion(
                                st.session_state.messages,
                                selected_model,
                                stream=False,
                                **extra_params
                            )
                            if response and hasattr(response, 'choices') and response.choices:
                                full_response = response.choices[0].message.content or ""
                                message_placeholder.markdown(full_response)
                    
                    if full_response:
                        message_placeholder.markdown(full_response)
                
                except Exception as e:
                    error_msg = f"âŒ éŒ¯èª¤: {str(e)}"
                    message_placeholder.markdown(error_msg)
                    full_response = error_msg
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
